---
layout:     post
title:      深入理解HashMap
subtitle:   源码分析
date:       2018-07-02
author:     PyP
header-img: img/post-bg-os-metro.jpg
catalog: 	 true
tags:
    - 哈希散列表
    - 红黑树
    - 多线程并发
    - 泊松分布
---

# 简介
HashMap主要用来存放<font color ="red">键值对</font>，基于哈希表，实现Map借口，是常用的java集合之一。
与HashTable的区别包括<font color ="red">不支持同步</font>，并且<font color ="red">允许Null值</font>作为key和value,但是<font color ="red">不可以由get()方法</font>判断是否存在null键，这样会报空指针异常，需要用containKey方法.如果想要线程安全，可以考虑使用<font color ="red">ConcurrentHashMap</font>.

***

# 结构分析
### jdk1.7
JDK1.8之前HashMap底层是通过<font color = "red">数组和链表</font>结合在一起使用的<font color = "red">链表散列</font>,HashMap通过Key的hashcode来计算hash值，当hashCode相同时，通过<font color ="red">“拉链法”</font>解决冲突。
<font color="red">"拉链法"</font>即是：将数组与链表结合，创建一个链表数组，数组每一位都是一个链表。若遇到“哈希冲突”，则将冲突的值加到链表中即可。对于添加操作，查找动作很快，仅仅需要一次寻址，时间复杂度为O(1),新的Entry将会被插入到链表的头部，对于查找操作，只需要遍历链表，然后通过key对象的equal方法逐一比对查找。在hashcode特别差的情况下，比如所有key的hashcode都相同，链表就会很长，时间复杂度就会退到O(n)

![jdk1.7](https://ws4.sinaimg.cn/large/006tKfTcly1fsvf0p3ddgj309o0bvt8s.jpg)

### jdk1.8

JDK1.8之后对于解决哈希冲突有了新的优化，当链表长度大于阀值（默认为8）时，将链表转化成红黑树，以减少搜索的时间。由于红黑树的特点，即是hashcode相同，时间复杂度也只需要O(log n),但是需要注意的是，key的对象必须正确的实现compare接口，否则其速度会比jdk1.7还要慢，其作用在于防御Hash Collision Dos 攻击。

![jdk1.8](https://ws3.sinaimg.cn/large/006tKfTcly1fsvf7g5mnej30fc0efdg4.jpg)

***
# 源码分析
### 类的属性
![类属性](https://ws1.sinaimg.cn/large/006tKfTcly1fsvfgpzc50j30mx0fuq6v.jpg)

* default_initial_capacity = 1<<4  
这是hashmap中的默认容量，即初始的数组长度为16
* loadFactor 加载因子  
加载因子是控制数组存放数据的疏密程度，loadFactor越趋近于1，数组中存放的数据（entry）也就越多，链表越长，访问越慢，loadfactor越趋近于0，会导致每次达到容量的一半就要进行扩容，慢慢的最终使用空间和未使用空间的差值会逐渐增加，空间利用率降低。 而为什么是0.75f？jdk1.7 API中这样讲到：
> As a general rule, the default load factor (.75)
> offers a good tradeoff between time and space costs. 
> Higher values decrease the space overhead but 
> increase the lookup cost (reflected in most of the
> operations of the HashMap class, including get and
> put). The expected number of entries in the map and
> its load factor should be taken into account when 
> setting its initial capacity, so as to minimize the
> number of rehash operations. If the initial capacity
> is greater than the maximum number of entries 
> divided by the load factor, no rehash operations will
> ever occur.

翻译后大意为:  
作为一般规则，默认负载因子（0.75）在时间和空间成本上提供了很好的折衷。较高的值会降低空间开销，但提高查找成本（体现在大多数的HashMap类的操作，包括get和put）。设置初始大小时，应该考虑预计的entry数在map及其负载系数，并且尽量减少rehash操作的次数。如果初始容量大于最大条目数除以负载因子，rehash操作将不会发生。

StackOverFlow有人回答：
![StackOverFlow截图](https://ws2.sinaimg.cn/large/006tKfTcly1fsvg23j92kj30jg0epaho.jpg)
百度翻译简单来说就是：  
通过预测桶是否为空，可以避免链接和分支预测。如果空的概率超过0.5，桶可能是空的。让s代表size，让n代表添加的键的数量，使用牛顿二项式定理，桶是空的概率是：
`P(0) = C(n, 0) * (1/s)^0 * (1 - 1/s)^(n - 0)`  
因此，一个桶可能是空的如果少于
`log(2)/log(s/(s - 1))`个键，当s达到无穷大，并且如果增加的key的数目是P(0)=0.5 ,那么N/S快速接近log(2):
`lim (log(2)/log(s/(s - 1)))/s as s -> infinity = log(2) ~ 0.693...`
也就是说可能小于0.75大于等于log(2)的factor都能提供更好的性能，而0.75很有可能只是凭空想象出来...  

* threshold  
threshold = capactiy * loadFactory , 当size>=threshold的时候，就要考虑对数组的扩容了。

* treeify_threshold = 8  
 当bucket上结点数大于这个值就会转换成红黑树。为什么是8？  
 原因如下：  
 在理想情况下，在随机哈希码下，在容器中的节点频率遵循泊松分布，其参数平均为约0.5，对于默认大小调整阀值为0.75，尽管由于粒度的调整而存在较大的差异。忽略方差，列表大小k的期望发生的(exp(-0.5) pow(0.5, k) / factorial(k)),取值和概率如下
 
 ![概率图](https://ws4.sinaimg.cn/large/006tKfTcly1fsvhfmr4zhj30690c7dge.jpg)  
 也就是说，bucket内的链表长度大于8的概率约乎于0，因此当长度大于8的时候可以进行树的转换  
 （PS：泊松分布-泊松分布的参数λ是单位时间(或单位面积)内随机事件的平均发生率。 泊松分布适合于描述单位时间内随机事件发生的次数。）
 
### 构造函数
HashMap包括四个构造函数
![构造函数](https://ws4.sinaimg.cn/large/006tNc79ly1fsvq9zbyjgj30me0efn0c.jpg)

### putMapEntries方法
此方法将一个Map的内容全部转移到新的Map里面，首先获取旧Map的长度，判断新的table是否初始化，若未初始化则初始化相关属性，根据公式计算旧Map的所对应的新数组的容量是否大于阀值，大于则通过tableSizeFor方法初始化阀值。若已初始化并且旧Map长度大于阀值，则进行resize方法操作，最后遍历旧Map内容放到新Map里面
![方法](https://ws4.sinaimg.cn/large/006tNc79ly1fsvrlew4hqj30hl0dnmzk.jpg)

### put方法
HashMap只提供put用于添加元素，而put只是调用了putVal方法。  
关于putVal方法简单概括几个分支点，一是根据(n-1)&hash为坐标判断该位置是否为null，其中n为数组的长度或者是resize后数组的长度，hash为根据key得到的hash值。二是在确定该位置存在元素，判断桶中的第一个元素（数组的结点）的hash，key和put的元素的属性值是否相同，若不相同判断是红黑树结点，或是链表结点。三是确定为链表结点，需要遍历链表每个结点与put的元素是否相同包括hash和key，并判断若插入后链表长度大于8，则进行转换树操作。若找到了与之相同的元素会在onlyIfAbsent为false或者该位置原元素的value值为null时，将新的value赋给它，最后判断是否达到阀值进行扩容。另外，evict（驱逐）在false的情况下table为创作模式。

	final V putVal(int hash, K key, V value, boolean  
	onlyIfAbsent,boolean evict) {  
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    // table未初始化或者长度为0，进行扩容
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    // (n - 1) & hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中)
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
    // 桶中已经存在元素
    else {
        Node<K,V> e; K k;
        // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
                // 将第一个元素赋值给e，用e来记录
                e = p;
        // hash值不相等，即key不相等；为红黑树结点
        else if (p instanceof TreeNode)
            // 放入树中
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        // 为链表结点
        else {
            // 在链表最末插入结点
            for (int binCount = 0; ; ++binCount) {
                // 到达链表的尾部
                if ((e = p.next) == null) {
                    // 在尾部插入新结点
                    p.next = newNode(hash, key, value, null);
                    // 结点数量达到阈值，转化为红黑树
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    // 跳出循环
                    break;
                }
                // 判断链表中结点的key值与插入的元素的key值是否相等
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    // 相等，跳出循环
                    break;
                // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表
                p = e;
            }
        }
        // 表示在桶中找到key值、hash值与插入元素相等的结点
        if (e != null) { 
            // 记录e的value
            V oldValue = e.value;
            // onlyIfAbsent为false或者旧值为null
            if (!onlyIfAbsent || oldValue == null)
                //用新值替换旧值
                e.value = value;
            // 访问后回调
            afterNodeAccess(e);
            // 返回旧值
            return oldValue;
        }
    }
    // 结构性修改
    ++modCount;
    // 实际大小大于阈值则扩容
    if (++size > threshold)
        resize();
    // 插入后回调
    afterNodeInsertion(evict);
    return null;
	}


### get方法
调用内部的getNode函数，参数是key和key的hash值，如果table已初始化，首先判断在（n-1）&hash位置，其中n为table的长度，的结点是否与已知相等，是则返回，不是则再次判断是在该位置下的树结点上还是链表结点上，若都没有则返回null。

	final Node<K,V> getNode(int hash, Object key) {
    Node<K,V>[] tab; Node<K,V> first, e; int n; K k;
    // table已经初始化，长度大于0，根据hash寻找table中的项也不为空
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (first = tab[(n - 1) & hash]) != null) {
        // 桶中第一项(数组元素)相等
        if (first.hash == hash && // always check first node
            ((k = first.key) == key || (key != null && key.equals(k))))
            return first;
        // 桶中不止一个结点
        if ((e = first.next) != null) {
            // 为红黑树结点
            if (first instanceof TreeNode)
                // 在红黑树中查找
                return ((TreeNode<K,V>)first).getTreeNode(hash, key);
            // 否则，在链表中查找
            do {
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            } while ((e = e.next) != null);
        }
    }
    return null;
	}


### resize方法
首先我们分析jdk1.7的resize源码

	void resize(int newCapacity) {   //传入新的容量  
    Entry[] oldTable = table;    //引用扩容前的Entry数组  
    int oldCapacity = oldTable.length;  
    if (oldCapacity == MAXIMUM_CAPACITY) {  //扩容前的数组大小如果已经达到最大(2^30)了  
        threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了  
        return;  
   		}  
  
    Entry[] newTable = new Entry[newCapacity];  //初始化一个新的Entry数组  
    transfer(newTable);                         //！！将数据转移到新的Entry数组里  
    table = newTable;                           //HashMap的table属性引用新的Entry数组  
    threshold = (int) (newCapacity * loadFactor);//修改阈值  
	}
	
这里就是用一个更大的数组来代替已有的容量小的数组，利用transfer方法将数组里的元素拷贝到新数组里面去。

	void transfer(Entry[] newTable) {  
    Entry[] src = table;                   //src引用了旧的Entry数组  
    int newCapacity = newTable.length;  
    for (int j = 0; j < src.length; j++) { //遍历旧的Entry数组  
        Entry<K, V> e = src[j];             //取得旧Entry数组的每个元素  
        if (e != null) {  
            src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象）  
            do {  
                Entry<K, V> next = e.next;  
                int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置  
                e.next = newTable[i]; //标记[1]  
                newTable[i] = e;      //将元素放在数组上  
                e = next;             //访问下一个Entry链上的元素  
            } while (e != null);  
        }  
    }  
	}  
	static int indexFor(int h, int length) {  
    return h & (length - 1);  
	}  
	
采用单链表头插入的方式，将newTable[i]位置上的元素赋值给e.nenxt，也就是说，同一位置上的新元素总会被放到链表的头部；在旧数组上同一条链上的元素，通过重新计算索引位置后，有可能被放到不同的位置上去。
其中。jdk1.7的缺点暴漏无疑，需要对每个entry元素进行rehash，效率可想而知很慢，jdk1.8对其优化下文详解，还要说的一点就是关于 **return h&(length-1)** 为什么返回的是这么一个值？ 
  
hashMap底层数组长度总是2的n次方，1.7中的构造函数有一句**cap<<=1** 而在1.8中有一个tableSizeFor方法，是一个静态方法，目的在于返回一个比给定整数大且最接近2的幂次方整数

	static final int tableSizeFor(int cap) { 
	int n = cap - 1; 
	n |= n >>> 1; 
	n |= n >>> 2; 
	n |= n >>> 4; 
	n |= n >>> 8; 
	n |= n >>> 16; 
	return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? 	MAXIMUM_CAPACITY : n + 1; 
	} 
我们用length为16（2^n）和15，放入15个元素，当len = 15 时，由下图可知发生了8次碰撞，浪费的空间非常大，有1、3、5、7、9、11、13、15处没有记录，也就是没有存放数据。这是因为他们在与14进行&运算时，得到的结果最后一位永远都是0，即0001、0011、0101、0111、1001、1011、1101、1111位置处是不可能存储数据的，空间减少，进一步增加碰撞几率，这样就会导致查询速度慢。而当length = 16时，length – 1 = 15 即1111，那么进行低位&运算时，值总是与原来hash值相同，而进行高位运算时，其值等于其低位值。

![比较图](https://ws2.sinaimg.cn/large/006tNc79ly1fswnm8egogj30gk0algnb.jpg)

**所以说当length为2的幂次方时，不同的hash值发生的碰撞概率比较小，这样会使数据在数组中分布均匀，查询速度提升**

OK回到正题，我们现在来分析1.8对resize的优化，由于我们使用的2次幂扩展，所以rehash过后的元素要不是在原位置上，要不是在原位置再移动2次幂的位置。下方注释为官方解释
 > Initializes or doubles table size.  If null, allocates in 
  accord with initial capacity target held in field threshold. 
  Otherwise, because we are using power-of-two expansion, the 
  elements from each bin must either stay at same index, or move 
  with a power of two offset in the new table. 
  
  看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。
  ![示例图1](https://ws2.sinaimg.cn/large/006tNc79ly1fswnvv6kvcj319c0ce761.jpg)
  元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化：
  ![示例图2](https://ws3.sinaimg.cn/large/006tNc79ly1fswnvwv5t8j30tk05mgmn.jpg)
  因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图：
  ![示例图3](https://ws4.sinaimg.cn/large/006tNc79ly1fswnw039z8j30z80kadlq.jpg)
  其中**e.hash&oldcap==0** 这条判断语句算的上是优化后的一大明显的地方，这句就是判断原来hash值新增的bit位是0或是1，它将链表数据散列到两个下标位置，当前位置概率为50%，高位位置概率也是50%，这样理论上链表的数据会均匀的分布到当前位置或者高位，另外为什么是&而不是%，除了运算速度快以外，如果使用%，那么在容量变为2倍的时候，需要rehash确定每个链表元素的位置。
  
***
# 并发问题

 在1.7之前的HashMap在多线程情况下put/get内容，会导致并发问题，可能会导致以下的一些问题：
 
 * **死循环**：对同一个hashMap作put操作，可能会导致两个或以上的线程同时作rehash操作
 * **put元素丢失**：主要问题出在addEntry方法的new Entry (hash, key, value, e)，如果两个线程都同时取得了e,则他们下一个元素都是e，然后赋值给table元素的时候有一个成功有一个丢失。
 * **put非空元素get为null**： 在transfer方法中：
 
		void transfer(Entry[] newTable) {
    	Entry[] src = table;
    	int newCapacity = newTable.length;
    	for (int j = 0; j < src.length; j++) {
        Entry e = src[j];
        if (e != null) {
            src[j] = null;
            do {
                Entry next = e.next;
                int i = indexFor(e.hash, newCapacity);
                e.next = newTable[i];
                newTable[i] = e;
                e = next;
            } while (e != null);
        }
    	}
		}
		
	在这个方法里，将旧数组赋值给src，遍历src，当src的元素非null时，就将src中的该元素置null，即将旧数组中的元素置null了，也就是这一句：

		if (e != null) {
        	src[j] = null;
此时若有get方法访问这个key，它取得的还是旧数组，当然就取不到其对应的value了。

* **产生循环链表**  
下面借用网上的分析   
（1）假设我们有两个线程。我用红色和浅蓝色标注了一下。我们再回头看一下我们的 transfer代码中的这个细节：

		do {
    		Entry<K,V> next = e.next; // <--假设线程一执行到这里就被调度挂起了
    		int i = indexFor(e.hash, newCapacity);
    		e.next = newTable[i];
    		newTable[i] = e;
    		e = next;
		} while (e != null);
	而我们的线程二执行完成了。于是我们有下面的这个样子。
	![图1](https://ws2.sinaimg.cn/large/006tNc79ly1fsxtf13s41j30h40c23yq.jpg)
	注意：因为Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。我们可以看到链表的顺序被反转后。   
（2）线程一被调度回来执行。先是执行 newTalbe[i] = e。然后是e = next，导致了e指向了key(7)。而下一次循环的next = e.next导致了next指向了key(3)。  
![图2](https://ws4.sinaimg.cn/large/006tNc79ly1fsxtetr8nvj30gf0agwen.jpg) 
（3）线程一接着工作。把key(7)摘下来，放到newTable[i]的第一个，然后把e和next往下移。  
![图3](https://ws3.sinaimg.cn/large/006tNc79ly1fsxtfhitx0j30hf0bfwer.jpg)
（4）环形链接出现。e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。
![图4](https://ws3.sinaimg.cn/large/006tNc79ly1fsxtetjeg9j30hb0azt8y.jpg)

###解决办法

*  **Hashtable代替HashMap**  
	Hashtable 是同步的，但由迭代器返回的 Iterator 和由所有 Hashtable 的“collection 视图方法”返回的 Collection 的 listIterator 方法都是快速失败的：在创建 Iterator 之后，如果从结构上对 Hashtable 进行修改，除非通过 Iterator 自身的移除或添加方法，否则在任何时间以任何方式对其进行修改，Iterator 都将抛出 ConcurrentModificationException。因此，面对并发的修改，Iterator 很快就会完全失败，而不冒在将来某个不确定的时间发生任意不确定行为的风险。由 Hashtable 的键和值方法返回的 Enumeration 不是快速失败的。  
	注意，迭代器的快速失败行为无法得到保证，因为一般来说，不可能对是否出现不同步并发修改做出任何硬性保证。快速失败迭代器会尽最大努力抛出 ConcurrentModificationException。因此，为提高这类迭代器的正确性而编写一个依赖于此异常的程序是错误做法：迭代器的快速失败行为应该仅用于检测程序错误。  
	(PS:什么是集合迭代器的快速失败行为？  
      以ArrayList为例，在多线程并发情况下，如果有一个线程在修改ArrayList集合的结构（插入、移除...），而另一个线程正在用迭代器遍历读取集合中的元素，此时将抛ConcurrentModificationException异常立即停止迭代遍历操作，而不需要等到遍历结束后再检查有没有出现问题；)
*  **Collection.synchronizedMap将HashMap包装起来**  
返回由指定映射支持的同步（线程安全的）映射。为了保证按顺序访问，必须通过返回的映射完成对底层映射的所有访问。在返回的映射或其任意 collection 视图上进行迭代时，强制用户手工在返回的映射上进行同步：

		Map m = Collections.synchronizedMap(new HashMap());
		...
		Set s = m.keySet();  // Needn't be in synchronized block
		...
		synchronized(m) {  // Synchronizing on m, not s!
		Iterator i = s.iterator(); // Must be in 	synchronized block
    	while (i.hasNext())
        foo(i.next());
		}
	不遵从此建议将导致无法确定的行为。如果指定映射是可序列化的，则返回的映射也将是可序列化的。
*  **使用ConcurrentHashMap**  
支持检索的完全并发和更新的所期望可调整并发的哈希表。此类遵守与 Hashtable 相同的功能规范，并且包括对应于 Hashtable 的每个方法的方法版本。不过，尽管所有操作都是线程安全的，但检索操作不必锁定，并且不支持以某种防止所有访问的方式锁定整个表。此类可以通过程序完全与 Hashtable 进行互操作，这取决于其线程安全，而与其同步细节无关。   
检索操作（包括 get）通常不会受阻塞，因此，可能与更新操作交迭（包括 put 和 remove）。检索会影响最近完成的更新操作的结果。对于一些聚合操作，比如 putAll 和 clear，并发检索可能只影响某些条目的插入和移除。类似地，在创建迭代器/枚举时或自此之后，Iterators 和 Enumerations 返回在某一时间点上影响哈希表状态的元素。它们不会抛出 ConcurrentModificationException。不过，迭代器被设计成每次仅由一个线程使用。

